{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of 89...\n",
      "Scraping page 2 of 89...\n",
      "Scraping page 3 of 89...\n",
      "Scraping page 4 of 89...\n",
      "Scraping page 5 of 89...\n",
      "Scraping page 6 of 89...\n",
      "Scraping page 7 of 89...\n",
      "Scraping page 8 of 89...\n",
      "Scraping page 9 of 89...\n",
      "Scraping page 10 of 89...\n",
      "Scraping page 11 of 89...\n",
      "Scraping page 12 of 89...\n",
      "Scraping page 13 of 89...\n",
      "Scraping page 14 of 89...\n",
      "Scraping page 15 of 89...\n",
      "Scraping page 16 of 89...\n",
      "Scraping page 17 of 89...\n",
      "Scraping page 18 of 89...\n",
      "Scraping page 19 of 89...\n",
      "Scraping page 20 of 89...\n",
      "Scraping page 21 of 89...\n",
      "Scraping page 22 of 89...\n",
      "Scraping page 23 of 89...\n",
      "Scraping page 24 of 89...\n",
      "Scraping page 25 of 89...\n",
      "Scraping page 26 of 89...\n",
      "Scraping page 27 of 89...\n",
      "Scraping page 28 of 89...\n",
      "Scraping page 29 of 89...\n",
      "Scraping page 30 of 89...\n",
      "Scraping page 31 of 89...\n",
      "Scraping page 32 of 89...\n",
      "Scraping page 33 of 89...\n",
      "Scraping page 34 of 89...\n",
      "Scraping page 35 of 89...\n",
      "Scraping page 36 of 89...\n",
      "Scraping page 37 of 89...\n",
      "Scraping page 38 of 89...\n",
      "Scraping page 39 of 89...\n",
      "Scraping page 40 of 89...\n",
      "Scraping page 41 of 89...\n",
      "Scraping page 42 of 89...\n",
      "Scraping page 43 of 89...\n",
      "Scraping page 44 of 89...\n",
      "Scraping page 45 of 89...\n",
      "Scraping page 46 of 89...\n",
      "Scraping page 47 of 89...\n",
      "Scraping page 48 of 89...\n",
      "Scraping page 49 of 89...\n",
      "Scraping page 50 of 89...\n",
      "Scraping page 51 of 89...\n",
      "Scraping page 52 of 89...\n",
      "Scraping page 53 of 89...\n",
      "Scraping page 54 of 89...\n",
      "Scraping page 55 of 89...\n",
      "Scraping page 56 of 89...\n",
      "Scraping page 57 of 89...\n",
      "Scraping page 58 of 89...\n",
      "Scraping page 59 of 89...\n",
      "Scraping page 60 of 89...\n",
      "Scraping page 61 of 89...\n",
      "Scraping page 62 of 89...\n",
      "Scraping page 63 of 89...\n",
      "Scraping page 64 of 89...\n",
      "Scraping page 65 of 89...\n",
      "Scraping page 66 of 89...\n",
      "Scraping page 67 of 89...\n",
      "Scraping page 68 of 89...\n",
      "Scraping page 69 of 89...\n",
      "Scraping page 70 of 89...\n",
      "Scraping page 71 of 89...\n",
      "Scraping page 72 of 89...\n",
      "Scraping page 73 of 89...\n",
      "Scraping page 74 of 89...\n",
      "Scraping page 75 of 89...\n",
      "Scraping page 76 of 89...\n",
      "Scraping page 77 of 89...\n",
      "Scraping page 78 of 89...\n",
      "Scraping page 79 of 89...\n",
      "Scraping page 80 of 89...\n",
      "Scraping page 81 of 89...\n",
      "Scraping page 82 of 89...\n",
      "Scraping page 83 of 89...\n",
      "Scraping page 84 of 89...\n",
      "Scraping page 85 of 89...\n",
      "Scraping page 86 of 89...\n",
      "Scraping page 87 of 89...\n",
      "Scraping page 88 of 89...\n",
      "Scraping page 89 of 89...\n",
      "Data has been saved to hamburg_apartment_listings.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to fetch and parse the HTML content of a single search results page to get ad URLs\n",
    "def get_ad_urls(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    ad_urls = []\n",
    "    for link in soup.find_all('a', class_='detailansicht'):\n",
    "        ad_url = link.get('href')\n",
    "        # Ensure the URL starts with 'https://www.wg-gesucht.de'\n",
    "        if ad_url.startswith('/'):\n",
    "            ad_url = 'https://www.wg-gesucht.de' + ad_url\n",
    "        elif not ad_url.startswith('http'):\n",
    "            ad_url = 'https://www.wg-gesucht.de/' + ad_url\n",
    "        ad_urls.append(ad_url)\n",
    "    return ad_urls\n",
    "\n",
    "# Function to fetch and parse the HTML content of an individual ad page\n",
    "def get_ad_details(ad_url):\n",
    "    response = requests.get(ad_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        title = soup.find('h1').text.strip()\n",
    "        details = []\n",
    "        price = \"\"\n",
    "        size = \"\"\n",
    "        apartment_size = \"\"\n",
    "        availability = \"\"\n",
    "        landlord = soup.find('div', class_='contact_info_name').text.strip() if soup.find('div', class_='contact_info_name') else ''\n",
    "        \n",
    "        for fact in soup.find_all('div', class_='col-xs-6 text-center'):\n",
    "            detail_label = fact.find('span', class_='key_fact_detail').text.strip()\n",
    "            detail_value = fact.find('b', class_='key_fact_value').text.strip()\n",
    "            \n",
    "            if detail_label == \"Gesamtmiete\":\n",
    "                price = detail_value\n",
    "            elif detail_label == \"Größe\" or detail_label == \"Zimmergröße\":\n",
    "                size = detail_value\n",
    "        \n",
    "        # Extracting apartment size\n",
    "        for item in soup.find_all('span', class_='section_panel_detail'):\n",
    "            if \"Wohnungsgröße\" in item.text:\n",
    "                apartment_size = item.text.split(\":\")[1].strip()\n",
    "        \n",
    "        # Extracting details section\n",
    "        details_section = soup.find('ul', class_='pl15 mb15')\n",
    "        if details_section:\n",
    "            for item in details_section.find_all('li'):\n",
    "                detail = item.text.strip()\n",
    "                details.append(detail)\n",
    "        details = \", \".join(details)\n",
    "        \n",
    "        # Extracting availability\n",
    "        availability_section = soup.find('div', class_='col-xs-6')\n",
    "        if availability_section:\n",
    "            online_status = availability_section.find('b', class_='noprint')\n",
    "            if online_status:\n",
    "                availability = online_status.text.strip()\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'details': details,\n",
    "            'price': price,\n",
    "            'size': size,\n",
    "            'apartment_size': apartment_size,\n",
    "            'availability': availability,\n",
    "            'landlord': landlord,\n",
    "            'url': ad_url\n",
    "        }\n",
    "    except AttributeError as e:\n",
    "        return {}\n",
    "\n",
    "# Base URL of the WG-Gesucht search results for Hamburg with pagination parameter\n",
    "base_url = 'https://www.wg-gesucht.de/wg-zimmer-und-1-zimmer-wohnungen-und-wohnungen-in-Hamburg.55.0+1+2.1.{}.html?offer_filter=1&city_id=55&sort_order=0&noDeact=1&categories%5B%5D=0&categories%5B%5D=1&categories%5B%5D=2&pagination=1&pu='\n",
    "\n",
    "# Initialize an empty list to store all ad details\n",
    "all_ads = []\n",
    "\n",
    "# Number of pages to scrape\n",
    "total_pages = 89\n",
    "\n",
    "# Loop through each page to get the ad URLs\n",
    "for page in range(1, total_pages + 1):\n",
    "    page_url = base_url.format(page)\n",
    "    print(f'Scraping page {page} of {total_pages}...')\n",
    "    ad_urls = get_ad_urls(page_url)\n",
    "    \n",
    "    # Loop through each ad URL to get the ad details\n",
    "    for ad_url in ad_urls:\n",
    "        print(f'Scraping ad: {ad_url}')\n",
    "        ad_details = get_ad_details(ad_url)\n",
    "        if ad_details:\n",
    "            all_ads.append(ad_details)\n",
    "        time.sleep(1)  # Adding delay to avoid overloading the server\n",
    "\n",
    "# Convert all ad details to a DataFrame\n",
    "df = pd.DataFrame(all_ads)\n",
    "\n",
    "# Save DataFrame to an Excel file\n",
    "df.to_excel('hamburg_apartment_listings.xlsx', index=False)\n",
    "\n",
    "print('Data has been saved to hamburg_apartment_listings.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
